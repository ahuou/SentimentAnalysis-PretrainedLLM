{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "bach",
   "name": "bach"
  },
  "language_info": {
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "version": "3.6.4",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "name": "python",
   "mimetype": "text/x-python"
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "sourceId": 239192,
     "sourceType": "datasetVersion",
     "datasetId": 100982
    },
    {
     "sourceId": 5383266,
     "sourceType": "datasetVersion",
     "datasetId": 2428302
    }
   ],
   "dockerImageVersionId": 30301,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": false
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "source": "import numpy as np\nimport pandas as pd \nimport nltk\nimport matplotlib.pyplot as plt\nfrom wordcloud import WordCloud,STOPWORDS\nfrom PIL import Image\nfrom sklearn.utils import resample\nimport seaborn as sns\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport re\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.probability import FreqDist\nfrom transformers import RobertaTokenizer\nfrom transformers import TFRobertaModel\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.layers import Dense,Dropout\nfrom tensorflow.keras.layers import Flatten\nfrom sklearn.metrics import precision_recall_curve\n\nimport tensorflow as tf\nfrom sklearn.metrics import roc_auc_score, roc_curve,auc\nfrom keras import regularizers\nfrom sklearn import preprocessing\nfrom sklearn.metrics import confusion_matrix,f1_score,classification_report\nfrom mlxtend.plotting import plot_confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import label_binarize\nfrom sklearn.metrics import average_precision_score\nfrom itertools import cycle\n\nlemma = WordNetLemmatizer()\nstopword = set(STOPWORDS)\nnltk.download('omw-1.4')\n%matplotlib inline\nimport nltk\nnltk.download('stopwords')\nimport nltk\nnltk.download('wordnet')\n\n\nfrom nltk.corpus import brown\nnltk.download(\"brown\")\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n",
   "metadata": {
    "id": "RTbL6P0DPT-l",
    "outputId": "ff792b6c-5e67-4439-a160-ff5b2e6f5051",
    "execution": {
     "iopub.status.busy": "2023-06-08T14:49:28.580465Z",
     "iopub.execute_input": "2023-06-08T14:49:28.580824Z",
     "iopub.status.idle": "2023-06-08T14:49:41.924737Z",
     "shell.execute_reply.started": "2023-06-08T14:49:28.580792Z",
     "shell.execute_reply": "2023-06-08T14:49:41.923673Z"
    },
    "trusted": true,
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "# Import Train Data",
   "metadata": {
    "id": "4WMfDUifPu9s"
   }
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Data preprocessing"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "%cd ..\n",
    "from data_retriever import data1, data2"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-12T11:45:44.981963Z",
     "start_time": "2024-06-12T11:45:44.965963Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def clean_text(tweet):\n",
    "    tweets = \" \".join(filter(lambda x: x[0]!= '@' , tweet.split()))\n",
    "    tweets = re.sub('[^a-zA-Z]', ' ', tweets)\n",
    "    tweets = tweets.lower()\n",
    "    tweets = tweets.split()\n",
    "    tweets = [word for word in tweets if not word in set(stopwords.words('english'))]\n",
    "    tweets = [lemma.lemmatize(word) for word in tweets]\n",
    "    tweets = \" \".join(tweets)\n",
    "    return tweets"
   ],
   "outputs": [],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-12T11:45:45.771184Z",
     "start_time": "2024-06-12T11:45:45.757670Z"
    }
   },
   "cell_type": "code",
   "source": "train_data = data2.astype(str).copy()",
   "outputs": [],
   "execution_count": 39
  },
  {
   "cell_type": "code",
   "source": "train_data['clean_tweet_text'] = train_data.Text.apply(clean_text)",
   "metadata": {
    "id": "EksOwZ3fPZCE",
    "execution": {
     "iopub.status.busy": "2023-06-08T14:50:12.942562Z",
     "iopub.execute_input": "2023-06-08T14:50:12.943209Z",
     "iopub.status.idle": "2023-06-08T14:51:15.385674Z",
     "shell.execute_reply.started": "2023-06-08T14:50:12.943167Z",
     "shell.execute_reply": "2023-06-08T14:51:15.384598Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-06-12T11:46:07.269536Z",
     "start_time": "2024-06-12T11:45:46.126947Z"
    }
   },
   "outputs": [],
   "execution_count": 40
  },
  {
   "cell_type": "code",
   "source": "print(train_data.head())",
   "metadata": {
    "id": "lFIZ299MPZGG",
    "outputId": "f66a7c7a-e86d-4925-f071-17590f293b5d",
    "execution": {
     "iopub.status.busy": "2023-06-08T14:51:15.387433Z",
     "iopub.execute_input": "2023-06-08T14:51:15.387861Z",
     "iopub.status.idle": "2023-06-08T14:51:15.39881Z",
     "shell.execute_reply.started": "2023-06-08T14:51:15.387821Z",
     "shell.execute_reply": "2023-06-08T14:51:15.397586Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-06-12T11:46:07.285044Z",
     "start_time": "2024-06-12T11:46:07.270529Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                Text Sentiment  \\\n",
      "0                                                nan   neutral   \n",
      "1                     Sage Act upgrade list tommorow  Positive   \n",
      "2  WAY HOMEGIRL BABY FUNERAL MAN HATE FUNERALS SH...  Negative   \n",
      "3  eye  true hazel eyeand brilliant  Regular feat...  Positive   \n",
      "4    ugh babe hugggzzz u  babe naamazed nga ako e...  Positive   \n",
      "\n",
      "                                    clean_tweet_text  \n",
      "0                                                nan  \n",
      "1                     sage act upgrade list tommorow  \n",
      "2  way homegirl baby funeral man hate funeral sho...  \n",
      "3  eye true hazel eyeand brilliant regular featur...  \n",
      "4  ugh babe hugggzzz u babe naamazed nga ako e ba...  \n"
     ]
    }
   ],
   "execution_count": 41
  },
  {
   "cell_type": "code",
   "source": "train_data=train_data.drop([\"Text\"],axis=1)",
   "metadata": {
    "id": "FgRRUAEqaOeb",
    "execution": {
     "iopub.status.busy": "2023-06-08T14:51:20.56572Z",
     "iopub.execute_input": "2023-06-08T14:51:20.566097Z",
     "iopub.status.idle": "2023-06-08T14:51:20.574943Z",
     "shell.execute_reply.started": "2023-06-08T14:51:20.566057Z",
     "shell.execute_reply": "2023-06-08T14:51:20.573839Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-06-12T11:49:19.122427Z",
     "start_time": "2024-06-12T11:49:19.114427Z"
    }
   },
   "outputs": [],
   "execution_count": 42
  },
  {
   "cell_type": "code",
   "source": "train_data.head()",
   "metadata": {
    "id": "gLDi1WypaOnG",
    "outputId": "022cbe18-5c8e-44aa-cef6-574105f4f31f",
    "execution": {
     "iopub.status.busy": "2023-06-08T14:51:20.576837Z",
     "iopub.execute_input": "2023-06-08T14:51:20.577147Z",
     "iopub.status.idle": "2023-06-08T14:51:20.590134Z",
     "shell.execute_reply.started": "2023-06-08T14:51:20.577119Z",
     "shell.execute_reply": "2023-06-08T14:51:20.588981Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-06-12T11:49:21.515839Z",
     "start_time": "2024-06-12T11:49:21.509840Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  Sentiment                                   clean_tweet_text\n",
       "0   neutral                                                nan\n",
       "1  Positive                     sage act upgrade list tommorow\n",
       "2  Negative  way homegirl baby funeral man hate funeral sho...\n",
       "3  Positive  eye true hazel eyeand brilliant regular featur...\n",
       "4  Positive  ugh babe hugggzzz u babe naamazed nga ako e ba..."
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>clean_tweet_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neutral</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Positive</td>\n",
       "      <td>sage act upgrade list tommorow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Negative</td>\n",
       "      <td>way homegirl baby funeral man hate funeral sho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Positive</td>\n",
       "      <td>eye true hazel eyeand brilliant regular featur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Positive</td>\n",
       "      <td>ugh babe hugggzzz u babe naamazed nga ako e ba...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 43
  },
  {
   "cell_type": "code",
   "source": [
    "max_len=128\n",
    "text_data=train_data[\"clean_tweet_text\"]\n",
    "label_data=train_data[\"Sentiment\"]"
   ],
   "metadata": {
    "id": "iIfp49hcbyhB",
    "execution": {
     "iopub.status.busy": "2023-06-08T14:51:20.591798Z",
     "iopub.execute_input": "2023-06-08T14:51:20.592181Z",
     "iopub.status.idle": "2023-06-08T14:51:20.598076Z",
     "shell.execute_reply.started": "2023-06-08T14:51:20.592142Z",
     "shell.execute_reply": "2023-06-08T14:51:20.596846Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-06-12T11:49:23.721480Z",
     "start_time": "2024-06-12T11:49:23.716477Z"
    }
   },
   "outputs": [],
   "execution_count": 44
  },
  {
   "cell_type": "markdown",
   "source": "# roberta_tokenizer",
   "metadata": {
    "id": "qdaPVMYbbbbO"
   }
  },
  {
   "cell_type": "code",
   "source": "roberta_tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")",
   "metadata": {
    "id": "RqTbI32waOtk",
    "execution": {
     "iopub.status.busy": "2023-06-08T14:51:20.610246Z",
     "iopub.execute_input": "2023-06-08T14:51:20.610689Z",
     "iopub.status.idle": "2023-06-08T14:51:25.541375Z",
     "shell.execute_reply.started": "2023-06-08T14:51:20.610653Z",
     "shell.execute_reply": "2023-06-08T14:51:25.540311Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-06-12T11:49:25.726813Z",
     "start_time": "2024-06-12T11:49:25.465488Z"
    }
   },
   "outputs": [],
   "execution_count": 45
  },
  {
   "cell_type": "markdown",
   "source": "# roberta_model",
   "metadata": {
    "id": "5n7J2DqbbjBT"
   }
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-12T11:49:29.614309Z",
     "start_time": "2024-06-12T11:49:29.600301Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import tensorflow as tf\n",
    "print(tf.reduce_sum(tf.random.normal([1000, 1000])))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(-453.73056, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "execution_count": 46
  },
  {
   "cell_type": "code",
   "source": [
    "from transformers import RobertaModel\n",
    "\n",
    "roberta_model = RobertaModel.from_pretrained(\"roberta-base\")"
   ],
   "metadata": {
    "id": "D4AugOATaO0f",
    "outputId": "5ff24a49-842f-408e-9695-da1838840e9f",
    "execution": {
     "iopub.status.busy": "2023-06-08T14:51:25.54285Z",
     "iopub.execute_input": "2023-06-08T14:51:25.543545Z",
     "iopub.status.idle": "2023-06-08T14:51:54.528313Z",
     "shell.execute_reply.started": "2023-06-08T14:51:25.543503Z",
     "shell.execute_reply": "2023-06-08T14:51:54.527226Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-06-12T11:57:09.376759Z",
     "start_time": "2024-06-12T11:56:24.432392Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7e8cbd3d802d4bd3a32a2986bee4b106"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "execution_count": 58
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-12T11:57:17.059763Z",
     "start_time": "2024-06-12T11:57:15.681978Z"
    }
   },
   "cell_type": "code",
   "source": "!pip show tensorflow",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: tensorflow\n",
      "Version: 2.10.1\n",
      "Summary: TensorFlow is an open source machine learning framework for everyone.\n",
      "Home-page: https://www.tensorflow.org/\n",
      "Author: Google Inc.\n",
      "Author-email: packages@tensorflow.org\n",
      "License: Apache 2.0\n",
      "Location: c:\\users\\alexh\\appdata\\roaming\\python\\python310\\site-packages\n",
      "Requires: grpcio, h5py, astunparse, typing-extensions, setuptools, tensorflow-io-gcs-filesystem, packaging, flatbuffers, tensorboard, tensorflow-estimator, six, termcolor, google-pasta, keras-preprocessing, keras, opt-einsum, numpy, protobuf, absl-py, wrapt, gast, libclang\n",
      "Required-by: \n"
     ]
    }
   ],
   "execution_count": 59
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-12T11:57:29.045910Z",
     "start_time": "2024-06-12T11:57:28.116306Z"
    }
   },
   "cell_type": "code",
   "source": "pip show transformers",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: transformers\n",
      "Version: 4.41.2\n",
      "Summary: State-of-the-art Machine Learning for JAX, PyTorch and TensorFlow\n",
      "Home-page: https://github.com/huggingface/transformers\n",
      "Author: The Hugging Face team (past and future) with the help of all our contributors (https://github.com/huggingface/transformers/graphs/contributors)\n",
      "Author-email: transformers@huggingface.co\n",
      "License: Apache 2.0 License\n",
      "Location: c:\\users\\alexh\\miniconda3\\envs\\bach\\lib\\site-packages\n",
      "Requires: filelock, huggingface-hub, numpy, packaging, pyyaml, regex, requests, safetensors, tokenizers, tqdm\n",
      "Required-by: \n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "execution_count": 60
  },
  {
   "cell_type": "markdown",
   "source": "# Sample Text",
   "metadata": {
    "id": "Gg_dnb_7nMXe"
   }
  },
  {
   "cell_type": "code",
   "source": "text_data[1000]",
   "metadata": {
    "id": "PR-jLsKJaPCy",
    "outputId": "58d8325a-dfa0-420c-b289-2b323223b58c",
    "execution": {
     "iopub.status.busy": "2023-06-08T14:51:54.529806Z",
     "iopub.execute_input": "2023-06-08T14:51:54.530782Z",
     "iopub.status.idle": "2023-06-08T14:51:54.56176Z",
     "shell.execute_reply.started": "2023-06-08T14:51:54.530737Z",
     "shell.execute_reply": "2023-06-08T14:51:54.56067Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-06-12T11:57:30.682821Z",
     "start_time": "2024-06-12T11:57:30.669815Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'stuck cried dismay'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 61
  },
  {
   "cell_type": "code",
   "source": "encode_data=roberta_tokenizer.encode_plus(text_data[1000],add_special_tokens = True,max_length =40,pad_to_max_length = True,truncation=True)",
   "metadata": {
    "id": "FGGUFczEaPMb",
    "outputId": "718d54a0-6963-4b6b-acd4-19e24fd9e841",
    "execution": {
     "iopub.status.busy": "2023-06-08T14:51:54.563496Z",
     "iopub.execute_input": "2023-06-08T14:51:54.563939Z",
     "iopub.status.idle": "2023-06-08T14:51:56.152577Z",
     "shell.execute_reply.started": "2023-06-08T14:51:54.563899Z",
     "shell.execute_reply": "2023-06-08T14:51:56.151382Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-06-12T11:57:31.424552Z",
     "start_time": "2024-06-12T11:57:31.413032Z"
    }
   },
   "outputs": [],
   "execution_count": 62
  },
  {
   "cell_type": "code",
   "source": "encode_data",
   "metadata": {
    "id": "c1BKS6huPZf_",
    "outputId": "11aec23f-8385-4ab6-b250-1c6f64590815",
    "execution": {
     "iopub.status.busy": "2023-06-08T14:51:56.154195Z",
     "iopub.execute_input": "2023-06-08T14:51:56.154582Z",
     "iopub.status.idle": "2023-06-08T14:51:56.165647Z",
     "shell.execute_reply.started": "2023-06-08T14:51:56.154551Z",
     "shell.execute_reply": "2023-06-08T14:51:56.164357Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-06-12T11:57:32.150622Z",
     "start_time": "2024-06-12T11:57:32.137597Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [0, 620, 5858, 16670, 22135, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 63
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-12T12:15:44.737187Z",
     "start_time": "2024-06-12T12:15:43.373658Z"
    }
   },
   "cell_type": "code",
   "source": "!python -m pip install transformers",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\alexh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (4.41.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\alexh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\alexh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (24.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\alexh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in c:\\users\\alexh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\alexh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (3.14.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\alexh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (0.4.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\alexh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (2024.5.15)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in c:\\users\\alexh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (0.23.3)\n",
      "Requirement already satisfied: requests in c:\\users\\alexh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\alexh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\alexh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (2024.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\alexh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\alexh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\alexh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\alexh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\alexh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (2024.6.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\alexh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (2.2.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.3; however, version 24.0 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\alexh\\AppData\\Local\\Programs\\Python\\Python310\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "execution_count": 77
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-12T12:16:32.662266Z",
     "start_time": "2024-06-12T12:16:31.601144Z"
    }
   },
   "cell_type": "code",
   "source": "pip install transformers",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\alexh\\miniconda3\\envs\\bach\\lib\\site-packages (4.42.0.dev0)\n",
      "Requirement already satisfied: filelock in c:\\users\\alexh\\miniconda3\\envs\\bach\\lib\\site-packages (from transformers) (3.14.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in c:\\users\\alexh\\miniconda3\\envs\\bach\\lib\\site-packages (from transformers) (0.23.3)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\alexh\\miniconda3\\envs\\bach\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\alexh\\miniconda3\\envs\\bach\\lib\\site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\alexh\\miniconda3\\envs\\bach\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\alexh\\miniconda3\\envs\\bach\\lib\\site-packages (from transformers) (2024.5.15)\n",
      "Requirement already satisfied: requests in c:\\users\\alexh\\miniconda3\\envs\\bach\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in c:\\users\\alexh\\miniconda3\\envs\\bach\\lib\\site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\alexh\\miniconda3\\envs\\bach\\lib\\site-packages (from transformers) (0.4.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\alexh\\miniconda3\\envs\\bach\\lib\\site-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\alexh\\miniconda3\\envs\\bach\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (2024.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\alexh\\miniconda3\\envs\\bach\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\alexh\\miniconda3\\envs\\bach\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\alexh\\miniconda3\\envs\\bach\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\alexh\\miniconda3\\envs\\bach\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\alexh\\miniconda3\\envs\\bach\\lib\\site-packages (from requests->transformers) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\alexh\\miniconda3\\envs\\bach\\lib\\site-packages (from requests->transformers) (2024.6.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "execution_count": 81
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-12T12:18:59.939471Z",
     "start_time": "2024-06-12T12:18:58.665618Z"
    }
   },
   "cell_type": "code",
   "source": "pip install transformers[tf]",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers[tf] in c:\\users\\alexh\\miniconda3\\envs\\bach\\lib\\site-packages (4.42.0.dev0)\n",
      "Requirement already satisfied: filelock in c:\\users\\alexh\\miniconda3\\envs\\bach\\lib\\site-packages (from transformers[tf]) (3.14.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in c:\\users\\alexh\\miniconda3\\envs\\bach\\lib\\site-packages (from transformers[tf]) (0.23.3)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\alexh\\miniconda3\\envs\\bach\\lib\\site-packages (from transformers[tf]) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\alexh\\miniconda3\\envs\\bach\\lib\\site-packages (from transformers[tf]) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\alexh\\miniconda3\\envs\\bach\\lib\\site-packages (from transformers[tf]) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\alexh\\miniconda3\\envs\\bach\\lib\\site-packages (from transformers[tf]) (2024.5.15)\n",
      "Requirement already satisfied: requests in c:\\users\\alexh\\miniconda3\\envs\\bach\\lib\\site-packages (from transformers[tf]) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in c:\\users\\alexh\\miniconda3\\envs\\bach\\lib\\site-packages (from transformers[tf]) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\alexh\\miniconda3\\envs\\bach\\lib\\site-packages (from transformers[tf]) (0.4.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\alexh\\miniconda3\\envs\\bach\\lib\\site-packages (from transformers[tf]) (4.66.4)\n",
      "Requirement already satisfied: tensorflow<2.16,>2.9 in c:\\users\\alexh\\miniconda3\\envs\\bach\\lib\\site-packages (from transformers[tf]) (2.10.1)\n",
      "Requirement already satisfied: onnxconverter-common in c:\\users\\alexh\\miniconda3\\envs\\bach\\lib\\site-packages (from transformers[tf]) (1.13.0)\n",
      "Requirement already satisfied: tf2onnx in c:\\users\\alexh\\miniconda3\\envs\\bach\\lib\\site-packages (from transformers[tf]) (1.14.0)\n",
      "Requirement already satisfied: tensorflow-text<2.16 in c:\\users\\alexh\\miniconda3\\envs\\bach\\lib\\site-packages (from transformers[tf]) (2.10.0)\n",
      "Requirement already satisfied: keras-nlp>=0.3.1 in c:\\users\\alexh\\miniconda3\\envs\\bach\\lib\\site-packages (from transformers[tf]) (0.12.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\alexh\\miniconda3\\envs\\bach\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.0->transformers[tf]) (2024.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\alexh\\miniconda3\\envs\\bach\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.0->transformers[tf]) (4.12.2)\n",
      "Requirement already satisfied: keras-core in c:\\users\\alexh\\miniconda3\\envs\\bach\\lib\\site-packages (from keras-nlp>=0.3.1->transformers[tf]) (0.1.7)\n",
      "Requirement already satisfied: absl-py in c:\\users\\alexh\\miniconda3\\envs\\bach\\lib\\site-packages (from keras-nlp>=0.3.1->transformers[tf]) (2.1.0)\n",
      "Requirement already satisfied: rich in c:\\users\\alexh\\miniconda3\\envs\\bach\\lib\\site-packages (from keras-nlp>=0.3.1->transformers[tf]) (13.7.1)\n",
      "Requirement already satisfied: dm-tree in c:\\users\\alexh\\miniconda3\\envs\\bach\\lib\\site-packages (from keras-nlp>=0.3.1->transformers[tf]) (0.1.8)\n",
      "Requirement already satisfied: kagglehub in c:\\users\\alexh\\miniconda3\\envs\\bach\\lib\\site-packages (from keras-nlp>=0.3.1->transformers[tf]) (0.2.6)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\alexh\\miniconda3\\envs\\bach\\lib\\site-packages (from tensorflow<2.16,>2.9->transformers[tf]) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in c:\\users\\alexh\\miniconda3\\envs\\bach\\lib\\site-packages (from tensorflow<2.16,>2.9->transformers[tf]) (2.0.7)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in c:\\users\\alexh\\miniconda3\\envs\\bach\\lib\\site-packages (from tensorflow<2.16,>2.9->transformers[tf]) (0.4.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\alexh\\miniconda3\\envs\\bach\\lib\\site-packages (from tensorflow<2.16,>2.9->transformers[tf]) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\alexh\\miniconda3\\envs\\bach\\lib\\site-packages (from tensorflow<2.16,>2.9->transformers[tf]) (3.11.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in c:\\users\\alexh\\miniconda3\\envs\\bach\\lib\\site-packages (from tensorflow<2.16,>2.9->transformers[tf]) (1.1.2)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\alexh\\miniconda3\\envs\\bach\\lib\\site-packages (from tensorflow<2.16,>2.9->transformers[tf]) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\alexh\\miniconda3\\envs\\bach\\lib\\site-packages (from tensorflow<2.16,>2.9->transformers[tf]) (3.3.0)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in c:\\users\\alexh\\miniconda3\\envs\\bach\\lib\\site-packages (from tensorflow<2.16,>2.9->transformers[tf]) (3.19.6)\n",
      "Requirement already satisfied: setuptools in c:\\users\\alexh\\miniconda3\\envs\\bach\\lib\\site-packages (from tensorflow<2.16,>2.9->transformers[tf]) (69.5.1)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\alexh\\miniconda3\\envs\\bach\\lib\\site-packages (from tensorflow<2.16,>2.9->transformers[tf]) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\alexh\\miniconda3\\envs\\bach\\lib\\site-packages (from tensorflow<2.16,>2.9->transformers[tf]) (2.4.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\alexh\\miniconda3\\envs\\bach\\lib\\site-packages (from tensorflow<2.16,>2.9->transformers[tf]) (1.16.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\alexh\\miniconda3\\envs\\bach\\lib\\site-packages (from tensorflow<2.16,>2.9->transformers[tf]) (0.31.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\alexh\\miniconda3\\envs\\bach\\lib\\site-packages (from tensorflow<2.16,>2.9->transformers[tf]) (1.64.1)\n",
      "Requirement already satisfied: tensorboard<2.11,>=2.10 in c:\\users\\alexh\\miniconda3\\envs\\bach\\lib\\site-packages (from tensorflow<2.16,>2.9->transformers[tf]) (2.10.1)\n",
      "Requirement already satisfied: tensorflow-estimator<2.11,>=2.10.0 in c:\\users\\alexh\\miniconda3\\envs\\bach\\lib\\site-packages (from tensorflow<2.16,>2.9->transformers[tf]) (2.10.0)\n",
      "Requirement already satisfied: keras<2.11,>=2.10.0 in c:\\users\\alexh\\miniconda3\\envs\\bach\\lib\\site-packages (from tensorflow<2.16,>2.9->transformers[tf]) (2.10.0)\n",
      "Requirement already satisfied: tensorflow-hub>=0.8.0 in c:\\users\\alexh\\miniconda3\\envs\\bach\\lib\\site-packages (from tensorflow-text<2.16->transformers[tf]) (0.16.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\alexh\\miniconda3\\envs\\bach\\lib\\site-packages (from tqdm>=4.27->transformers[tf]) (0.4.6)\n",
      "Requirement already satisfied: onnx in c:\\users\\alexh\\miniconda3\\envs\\bach\\lib\\site-packages (from onnxconverter-common->transformers[tf]) (1.12.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\alexh\\miniconda3\\envs\\bach\\lib\\site-packages (from requests->transformers[tf]) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\alexh\\miniconda3\\envs\\bach\\lib\\site-packages (from requests->transformers[tf]) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\alexh\\miniconda3\\envs\\bach\\lib\\site-packages (from requests->transformers[tf]) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\alexh\\miniconda3\\envs\\bach\\lib\\site-packages (from requests->transformers[tf]) (2024.6.2)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\alexh\\miniconda3\\envs\\bach\\lib\\site-packages (from astunparse>=1.6.0->tensorflow<2.16,>2.9->transformers[tf]) (0.43.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\alexh\\miniconda3\\envs\\bach\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow<2.16,>2.9->transformers[tf]) (2.30.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\alexh\\miniconda3\\envs\\bach\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow<2.16,>2.9->transformers[tf]) (0.4.6)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\alexh\\miniconda3\\envs\\bach\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow<2.16,>2.9->transformers[tf]) (3.6)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\users\\alexh\\miniconda3\\envs\\bach\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow<2.16,>2.9->transformers[tf]) (0.6.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\alexh\\miniconda3\\envs\\bach\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow<2.16,>2.9->transformers[tf]) (1.8.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\alexh\\miniconda3\\envs\\bach\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow<2.16,>2.9->transformers[tf]) (3.0.3)\n",
      "Requirement already satisfied: tf-keras>=2.14.1 in c:\\users\\alexh\\miniconda3\\envs\\bach\\lib\\site-packages (from tensorflow-hub>=0.8.0->tensorflow-text<2.16->transformers[tf]) (2.15.0)\n",
      "Requirement already satisfied: namex in c:\\users\\alexh\\miniconda3\\envs\\bach\\lib\\site-packages (from keras-core->keras-nlp>=0.3.1->transformers[tf]) (0.0.8)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\alexh\\miniconda3\\envs\\bach\\lib\\site-packages (from rich->keras-nlp>=0.3.1->transformers[tf]) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\alexh\\miniconda3\\envs\\bach\\lib\\site-packages (from rich->keras-nlp>=0.3.1->transformers[tf]) (2.18.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\alexh\\miniconda3\\envs\\bach\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow<2.16,>2.9->transformers[tf]) (5.3.3)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\alexh\\miniconda3\\envs\\bach\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow<2.16,>2.9->transformers[tf]) (0.4.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\alexh\\miniconda3\\envs\\bach\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow<2.16,>2.9->transformers[tf]) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\alexh\\miniconda3\\envs\\bach\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow<2.16,>2.9->transformers[tf]) (2.0.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in c:\\users\\alexh\\miniconda3\\envs\\bach\\lib\\site-packages (from markdown>=2.6.8->tensorboard<2.11,>=2.10->tensorflow<2.16,>2.9->transformers[tf]) (7.1.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\alexh\\miniconda3\\envs\\bach\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras-nlp>=0.3.1->transformers[tf]) (0.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\alexh\\miniconda3\\envs\\bach\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.11,>=2.10->tensorflow<2.16,>2.9->transformers[tf]) (2.1.5)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\alexh\\miniconda3\\envs\\bach\\lib\\site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.11,>=2.10->tensorflow<2.16,>2.9->transformers[tf]) (3.19.2)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in c:\\users\\alexh\\miniconda3\\envs\\bach\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow<2.16,>2.9->transformers[tf]) (0.6.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\alexh\\miniconda3\\envs\\bach\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow<2.16,>2.9->transformers[tf]) (3.2.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "execution_count": 85
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-12T12:19:04.240460Z",
     "start_time": "2024-06-12T12:19:04.178527Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import transformers\n",
    "from transformers import pipeline\n",
    "print(pipeline('sentiment-analysis')('I hate you'))"
   ],
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Failed to import transformers.pipelines because of the following error (look up to see its traceback):\nNo module named 'transformers.pipelines'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "File \u001B[1;32m~\\miniconda3\\envs\\bach\\lib\\site-packages\\transformers\\utils\\import_utils.py:1535\u001B[0m, in \u001B[0;36m_LazyModule._get_module\u001B[1;34m(self, module_name)\u001B[0m\n\u001B[0;32m   1534\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m-> 1535\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mimportlib\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mimport_module\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m.\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mmodule_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;18;43m__name__\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1536\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\bach\\lib\\importlib\\__init__.py:126\u001B[0m, in \u001B[0;36mimport_module\u001B[1;34m(name, package)\u001B[0m\n\u001B[0;32m    125\u001B[0m             \u001B[38;5;28;01mbreak\u001B[39;00m\n\u001B[1;32m--> 126\u001B[0m         level \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m    127\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m _bootstrap\u001B[38;5;241m.\u001B[39m_gcd_import(name[level:], package, level)\n",
      "File \u001B[1;32m<frozen importlib._bootstrap>:1050\u001B[0m, in \u001B[0;36m_gcd_import\u001B[1;34m(name, package, level)\u001B[0m\n",
      "File \u001B[1;32m<frozen importlib._bootstrap>:1027\u001B[0m, in \u001B[0;36m_find_and_load\u001B[1;34m(name, import_)\u001B[0m\n",
      "File \u001B[1;32m<frozen importlib._bootstrap>:1004\u001B[0m, in \u001B[0;36m_find_and_load_unlocked\u001B[1;34m(name, import_)\u001B[0m\n",
      "\u001B[1;31mModuleNotFoundError\u001B[0m: No module named 'transformers.pipelines'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[86], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtransformers\u001B[39;00m\n\u001B[1;32m----> 2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtransformers\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m pipeline\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28mprint\u001B[39m(pipeline(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124msentiment-analysis\u001B[39m\u001B[38;5;124m'\u001B[39m)(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mI hate you\u001B[39m\u001B[38;5;124m'\u001B[39m))\n",
      "File \u001B[1;32m<frozen importlib._bootstrap>:1075\u001B[0m, in \u001B[0;36m_handle_fromlist\u001B[1;34m(module, fromlist, import_, recursive)\u001B[0m\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\bach\\lib\\site-packages\\transformers\\utils\\import_utils.py:1525\u001B[0m, in \u001B[0;36m_LazyModule.__getattr__\u001B[1;34m(self, name)\u001B[0m\n\u001B[0;32m   1523\u001B[0m     value \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_module(name)\n\u001B[0;32m   1524\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m name \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_class_to_module\u001B[38;5;241m.\u001B[39mkeys():\n\u001B[1;32m-> 1525\u001B[0m     module \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_get_module\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_class_to_module\u001B[49m\u001B[43m[\u001B[49m\u001B[43mname\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1526\u001B[0m     value \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mgetattr\u001B[39m(module, name)\n\u001B[0;32m   1527\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\bach\\lib\\site-packages\\transformers\\utils\\import_utils.py:1537\u001B[0m, in \u001B[0;36m_LazyModule._get_module\u001B[1;34m(self, module_name)\u001B[0m\n\u001B[0;32m   1535\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m importlib\u001B[38;5;241m.\u001B[39mimport_module(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m module_name, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m)\n\u001B[0;32m   1536\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m-> 1537\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[0;32m   1538\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFailed to import \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmodule_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m because of the following error (look up to see its\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1539\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m traceback):\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00me\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1540\u001B[0m     ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01me\u001B[39;00m\n",
      "\u001B[1;31mRuntimeError\u001B[0m: Failed to import transformers.pipelines because of the following error (look up to see its traceback):\nNo module named 'transformers.pipelines'"
     ]
    }
   ],
   "execution_count": 86
  },
  {
   "cell_type": "code",
   "source": [
    "test_input_id=np.asarray(encode_data['input_ids'])\n",
    "test_attention_mask=np.asarray(encode_data['attention_mask'])\n",
    "output_data=roberta_model([test_input_id.reshape(1,-1),test_attention_mask.reshape(1,-1)])\n",
    "type(output_data)"
   ],
   "metadata": {
    "id": "e9Oul71epI0R",
    "outputId": "1f630b50-9a89-41a5-f532-1ab78cca25fd",
    "execution": {
     "iopub.status.busy": "2023-06-08T14:51:56.167868Z",
     "iopub.execute_input": "2023-06-08T14:51:56.168314Z",
     "iopub.status.idle": "2023-06-08T14:51:56.299654Z",
     "shell.execute_reply.started": "2023-06-08T14:51:56.168272Z",
     "shell.execute_reply": "2023-06-08T14:51:56.298658Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-06-12T12:10:20.758319Z",
     "start_time": "2024-06-12T12:10:20.705148Z"
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not tuple",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[70], line 3\u001B[0m\n\u001B[0;32m      1\u001B[0m test_input_id\u001B[38;5;241m=\u001B[39mnp\u001B[38;5;241m.\u001B[39masarray(encode_data[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124minput_ids\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[0;32m      2\u001B[0m test_attention_mask\u001B[38;5;241m=\u001B[39mnp\u001B[38;5;241m.\u001B[39masarray(encode_data[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mattention_mask\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[1;32m----> 3\u001B[0m output_data\u001B[38;5;241m=\u001B[39m\u001B[43mroberta_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\u001B[43mtest_input_id\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreshape\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43mtest_attention_mask\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreshape\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28mtype\u001B[39m(output_data)\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\bach\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1530\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1531\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1532\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\bach\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1536\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1537\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1538\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1539\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1540\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1541\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1543\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1544\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\bach\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:779\u001B[0m, in \u001B[0;36mRobertaModel.forward\u001B[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[0;32m    777\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mYou cannot specify both input_ids and inputs_embeds at the same time\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    778\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m input_ids \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m--> 779\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwarn_if_padding_and_no_attention_mask\u001B[49m\u001B[43m(\u001B[49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    780\u001B[0m     input_shape \u001B[38;5;241m=\u001B[39m input_ids\u001B[38;5;241m.\u001B[39msize()\n\u001B[0;32m    781\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m inputs_embeds \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\bach\\lib\\site-packages\\transformers\\modeling_utils.py:4457\u001B[0m, in \u001B[0;36mPreTrainedModel.warn_if_padding_and_no_attention_mask\u001B[1;34m(self, input_ids, attention_mask)\u001B[0m\n\u001B[0;32m   4454\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m\n\u001B[0;32m   4456\u001B[0m \u001B[38;5;66;03m# Check only the first and last input IDs to reduce overhead.\u001B[39;00m\n\u001B[1;32m-> 4457\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39mpad_token_id \u001B[38;5;129;01min\u001B[39;00m \u001B[43minput_ids\u001B[49m\u001B[43m[\u001B[49m\u001B[43m:\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m]\u001B[49m:\n\u001B[0;32m   4458\u001B[0m     warn_string \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m   4459\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mWe strongly recommend passing in an `attention_mask` since your input_ids may be padded. See \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   4460\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhttps://huggingface.co/docs/transformers/troubleshooting\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   4461\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m#incorrect-output-when-padding-tokens-arent-masked.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   4462\u001B[0m     )\n\u001B[0;32m   4464\u001B[0m     \u001B[38;5;66;03m# If the pad token is equal to either BOS, EOS, or SEP, we do not know whether the user should use an\u001B[39;00m\n\u001B[0;32m   4465\u001B[0m     \u001B[38;5;66;03m# attention_mask or not. In this case, we should still show a warning because this is a rare case.\u001B[39;00m\n",
      "\u001B[1;31mTypeError\u001B[0m: list indices must be integers or slices, not tuple"
     ]
    }
   ],
   "execution_count": 70
  },
  {
   "cell_type": "code",
   "source": "output_data",
   "metadata": {
    "id": "brkCVXRYpJdE",
    "outputId": "2017c028-dc99-45d5-8129-b1b4f3f057a2",
    "execution": {
     "iopub.status.busy": "2023-06-08T14:51:56.301455Z",
     "iopub.execute_input": "2023-06-08T14:51:56.301902Z",
     "iopub.status.idle": "2023-06-08T14:51:56.325282Z",
     "shell.execute_reply.started": "2023-06-08T14:51:56.301857Z",
     "shell.execute_reply": "2023-06-08T14:51:56.324114Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "roberta_tokenizer.decode(encode_data[\"input_ids\"])",
   "metadata": {
    "id": "laNnYyo1pJhj",
    "outputId": "8a88369f-8a42-4ebb-da2c-dfc067863395",
    "execution": {
     "iopub.status.busy": "2023-06-08T14:51:56.326996Z",
     "iopub.execute_input": "2023-06-08T14:51:56.327408Z",
     "iopub.status.idle": "2023-06-08T14:51:57.532854Z",
     "shell.execute_reply.started": "2023-06-08T14:51:56.327368Z",
     "shell.execute_reply": "2023-06-08T14:51:57.531656Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "final_text_data=train_data[\"clean_tweet_text\"]\nfinal_label_data=np.array(label_data)",
   "metadata": {
    "id": "uuM-HefYpJl8",
    "execution": {
     "iopub.status.busy": "2023-06-08T14:51:57.53455Z",
     "iopub.execute_input": "2023-06-08T14:51:57.535843Z",
     "iopub.status.idle": "2023-06-08T14:51:57.541252Z",
     "shell.execute_reply.started": "2023-06-08T14:51:57.53579Z",
     "shell.execute_reply": "2023-06-08T14:51:57.540023Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "def roberta_encode(final_text_data,max_len):\n    input_ids=[]\n    attention_masks=[]\n    \n    for i in range(len(final_text_data)):\n        encode_data=roberta_tokenizer.encode_plus(final_text_data[i],add_special_tokens=True,max_length=max_len,pad_to_max_length=True,return_attention_mask=True)\n        input_ids.append(encode_data['input_ids'])\n        attention_masks.append(encode_data[\"attention_mask\"])\n    return np.array(input_ids),np.array(attention_masks)\n        ",
   "metadata": {
    "id": "DCRWV-gRpJq2",
    "execution": {
     "iopub.status.busy": "2023-06-08T14:51:57.543425Z",
     "iopub.execute_input": "2023-06-08T14:51:57.544417Z",
     "iopub.status.idle": "2023-06-08T14:51:57.553051Z",
     "shell.execute_reply.started": "2023-06-08T14:51:57.544377Z",
     "shell.execute_reply": "2023-06-08T14:51:57.552055Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "text_input_ids,text_attention_masks = roberta_encode(final_text_data,max_len)",
   "metadata": {
    "id": "CZ5nCIlapJur",
    "outputId": "d26e651a-717c-49ab-cb95-64ca55801455",
    "execution": {
     "iopub.status.busy": "2023-06-08T14:51:57.555Z",
     "iopub.execute_input": "2023-06-08T14:51:57.555447Z",
     "iopub.status.idle": "2023-06-08T14:52:11.360111Z",
     "shell.execute_reply.started": "2023-06-08T14:51:57.555398Z",
     "shell.execute_reply": "2023-06-08T14:52:11.358978Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "print('Text Input Ids Shape {} \\nText Input Attention Mask Shape {} \\nLabel Data shape {}'.format(text_input_ids.shape,text_attention_masks.shape,final_label_data.shape))",
   "metadata": {
    "id": "ovMvAGgApJyB",
    "outputId": "361426d8-a9b9-466e-890d-eee27269530e",
    "execution": {
     "iopub.status.busy": "2023-06-08T14:52:11.361866Z",
     "iopub.execute_input": "2023-06-08T14:52:11.362244Z",
     "iopub.status.idle": "2023-06-08T14:52:11.372515Z",
     "shell.execute_reply.started": "2023-06-08T14:52:11.362204Z",
     "shell.execute_reply": "2023-06-08T14:52:11.3713Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "X_train_input,X_test_input,Y_train_label,Y_test_label,train_mask,test_mask=train_test_split(text_input_ids,final_label_data,text_attention_masks,test_size=0.2,random_state=42,shuffle=True)",
   "metadata": {
    "id": "UKIjDuMspJ1g",
    "execution": {
     "iopub.status.busy": "2023-06-08T14:52:11.37423Z",
     "iopub.execute_input": "2023-06-08T14:52:11.375539Z",
     "iopub.status.idle": "2023-06-08T14:52:11.404837Z",
     "shell.execute_reply.started": "2023-06-08T14:52:11.375496Z",
     "shell.execute_reply": "2023-06-08T14:52:11.403658Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "print('Train input shape {}\\nTest input shape {}\\nTrain label shape {}\\nTest label shape {}\\nTrain attention mask shape {}\\nTest attention mask shape {}'.format(X_train_input.shape,X_test_input.shape,Y_train_label.shape,Y_test_label.shape,train_mask.shape,test_mask.shape))",
   "metadata": {
    "id": "zqOK0UrPpJ7B",
    "outputId": "44cd5443-64ba-4ac7-9748-7c27f973c82b",
    "execution": {
     "iopub.status.busy": "2023-06-08T14:52:11.408985Z",
     "iopub.execute_input": "2023-06-08T14:52:11.409319Z",
     "iopub.status.idle": "2023-06-08T14:52:11.415926Z",
     "shell.execute_reply.started": "2023-06-08T14:52:11.409289Z",
     "shell.execute_reply": "2023-06-08T14:52:11.414494Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# Create Roberta Model",
   "metadata": {
    "id": "SyrrDENuy_4R"
   }
  },
  {
   "cell_type": "code",
   "source": "def Create_Roberta_Model():\n    input_ids=tf.keras.Input(shape=(max_len,),dtype=\"int32\")\n    attention_masks=tf.keras.Input(shape=(max_len,),dtype=\"int32\")\n    \n    roberta_model = TFRobertaModel.from_pretrained(\"roberta-base\")\n    output_dim = roberta_model(input_ids=input_ids, attention_mask=attention_masks)[0][:,0,:]\n    \n    dense_layer = tf.keras.layers.Dense(128, activation=\"relu\")(output_dim)\n    dropout = tf.keras.layers.Dropout(0.5)(dense_layer)\n    final_layer = tf.keras.layers.Dense(2, activation='softmax')(dropout)\n\n    model = tf.keras.models.Model(inputs=[input_ids, attention_masks], outputs=final_layer)\n    \n    return model\n",
   "metadata": {
    "id": "5xKFWC6ATJPn",
    "execution": {
     "iopub.status.busy": "2023-06-08T14:52:11.418666Z",
     "iopub.execute_input": "2023-06-08T14:52:11.418977Z",
     "iopub.status.idle": "2023-06-08T14:52:11.431356Z",
     "shell.execute_reply.started": "2023-06-08T14:52:11.418949Z",
     "shell.execute_reply": "2023-06-08T14:52:11.430291Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "model=Create_Roberta_Model()\nloss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n\noptimizer = tf.keras.optimizers.Adam(lr=1e-5)\n\nmodel.compile(loss=loss,optimizer=optimizer, metrics=[\"accuracy\"])\n",
   "metadata": {
    "id": "HfWpP1nRpKIx",
    "outputId": "2a6f624b-49aa-46f0-c8be-163afbd65fae",
    "execution": {
     "iopub.status.busy": "2023-06-08T14:52:11.43273Z",
     "iopub.execute_input": "2023-06-08T14:52:11.433791Z",
     "iopub.status.idle": "2023-06-08T14:52:19.99564Z",
     "shell.execute_reply.started": "2023-06-08T14:52:11.433742Z",
     "shell.execute_reply": "2023-06-08T14:52:19.994556Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "history=model.fit([X_train_input,train_mask],Y_train_label,batch_size=42,epochs=10,validation_data=([X_test_input,test_mask],Y_test_label))",
   "metadata": {
    "id": "d1LGBF0tpKMM",
    "outputId": "0e669998-9ddb-43f2-be5b-b550d79e73ea",
    "execution": {
     "iopub.status.busy": "2023-06-08T14:52:19.997001Z",
     "iopub.execute_input": "2023-06-08T14:52:19.997402Z",
     "iopub.status.idle": "2023-06-08T15:59:47.67389Z",
     "shell.execute_reply.started": "2023-06-08T14:52:19.997363Z",
     "shell.execute_reply": "2023-06-08T15:59:47.672901Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "def model_loss_and_accuracy(history):\n    \n    fig=plt.figure(figsize=(15,15))\n    plt.subplot(221)\n    plt.plot(history.history[\"accuracy\"],marker=\"o\",linestyle=\":\",markersize=10,color=\"m\",label=\"accuracy\")\n    plt.plot(history.history[\"val_accuracy\"],marker=\"D\",linestyle=\":\",markersize=10,color=\"b\",label=\"val_accuracy\")\n    plt.title(\"Model Accuracy\\n\",fontsize=20,color=\"darkorange\")\n    plt.xlabel(\"Number of Epochs\",color=\"midnightblue\",fontsize=15)\n    plt.ylabel(\"Accuracy\",color=\"midnightblue\",fontsize=15)\n    plt.grid(color = 'green', linestyle = '--', linewidth = 1)\n    plt.legend(loc=\"best\")\n    plt.tight_layout()\n    \n  \n    plt.subplot(222)\n    plt.plot(history.history[\"accuracy\"],marker=\"v\",linestyle=\"-.\",markersize=10,color=\"g\",label=\"accuracy\")\n    plt.plot(history.history[\"val_accuracy\"],marker=\"s\",linestyle=\"--\",markersize=10,color=\"r\",label=\"val_accuracy\")\n    plt.title(\"Model Loss\\n\",fontsize=20,color=\"limegreen\")\n    plt.xlabel(\"Number of Epochs\",color=\"midnightblue\",fontsize=15)\n    plt.ylabel(\"Loss\",color=\"midnightblue\",fontsize=15)\n    plt.grid(color = 'green', linestyle = '--', linewidth = 1)\n    plt.legend(loc=\"best\")\n    plt.tight_layout()\n    plt.show()",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-06-08T15:59:47.675766Z",
     "iopub.execute_input": "2023-06-08T15:59:47.67614Z",
     "iopub.status.idle": "2023-06-08T15:59:47.689016Z",
     "shell.execute_reply.started": "2023-06-08T15:59:47.676104Z",
     "shell.execute_reply": "2023-06-08T15:59:47.687651Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "model_loss_and_accuracy(history)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-06-08T15:59:47.690566Z",
     "iopub.execute_input": "2023-06-08T15:59:47.691061Z",
     "iopub.status.idle": "2023-06-08T15:59:48.133763Z",
     "shell.execute_reply.started": "2023-06-08T15:59:47.691002Z",
     "shell.execute_reply": "2023-06-08T15:59:48.132792Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# F1 Score",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def plot_f1_score(history):\n    f1_scores = []\n    for epoch in range(len(history.history['val_loss'])):\n        model.set_weights(history.model.get_weights())\n        predictions = model.predict([X_test_input, test_mask])\n        predictions = [round(p[0]) for p in predictions]\n    \n        f1_scores.append(f1_score(Y_test_label, predictions, average='weighted'))\n    plt.figure(figsize=(10,10))\n    plt.plot(range(len(history.history['val_loss'])), f1_scores, label='F1 score',marker=\"*\",color=\"g\",linestyle=\"--\",linewidth=4,markersize=8,markerfacecolor=\"r\")\n    plt.title(\"F1 Score\\n\",color=\"black\",fontsize=20)\n    plt.xlabel('Epochs',fontsize=15,color=\"black\")\n    plt.ylabel('F1 score',fontsize=15,color=\"black\")\n    plt.legend()\n    plt.show()\n\n\nplot_f1_score(history)\n",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-06-08T15:59:48.137061Z",
     "iopub.execute_input": "2023-06-08T15:59:48.137362Z",
     "iopub.status.idle": "2023-06-08T16:06:07.982369Z",
     "shell.execute_reply.started": "2023-06-08T15:59:48.137334Z",
     "shell.execute_reply": "2023-06-08T16:06:07.981408Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# Label Names",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "label_name=[\"Positive\",\"Racist and Sexist\"]",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-06-08T16:06:07.984058Z",
     "iopub.execute_input": "2023-06-08T16:06:07.98453Z",
     "iopub.status.idle": "2023-06-08T16:06:07.989632Z",
     "shell.execute_reply.started": "2023-06-08T16:06:07.984492Z",
     "shell.execute_reply": "2023-06-08T16:06:07.988677Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "pred=model.predict([X_test_input, test_mask])\nprediction=np.argmax(pred, axis=1)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-06-08T16:06:07.990886Z",
     "iopub.execute_input": "2023-06-08T16:06:07.991738Z",
     "iopub.status.idle": "2023-06-08T16:06:40.6963Z",
     "shell.execute_reply.started": "2023-06-08T16:06:07.991699Z",
     "shell.execute_reply": "2023-06-08T16:06:40.695204Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# Accuracy Score",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "print(\"Accuracy Score is\",accuracy_score(Y_test_label,prediction))",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-06-08T16:06:40.698062Z",
     "iopub.execute_input": "2023-06-08T16:06:40.69845Z",
     "iopub.status.idle": "2023-06-08T16:06:40.705539Z",
     "shell.execute_reply.started": "2023-06-08T16:06:40.698412Z",
     "shell.execute_reply": "2023-06-08T16:06:40.704422Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# ROC AUC Score",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "print(\"ROC AUC Score is {}\".format(roc_auc_score(Y_test_label, pred[:,1])))",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-06-08T16:06:40.707308Z",
     "iopub.execute_input": "2023-06-08T16:06:40.708016Z",
     "iopub.status.idle": "2023-06-08T16:06:40.71916Z",
     "shell.execute_reply.started": "2023-06-08T16:06:40.707979Z",
     "shell.execute_reply": "2023-06-08T16:06:40.717897Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# Average Precision Score",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Get the predicted probabilities for the positive class\npred_probs = model.predict([X_test_input, test_mask])[:,1]\n\n# Binarize the test labels\nY_test_binarized = (Y_test_label == 1).astype(int)\n\n# Calculate the average precision score\naverage_precision = average_precision_score(Y_test_binarized, pred_probs)\n\nprint(\"Average Precision Score :\", average_precision)\n",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-06-08T16:06:40.720829Z",
     "iopub.execute_input": "2023-06-08T16:06:40.721181Z",
     "iopub.status.idle": "2023-06-08T16:07:21.721875Z",
     "shell.execute_reply.started": "2023-06-08T16:06:40.721146Z",
     "shell.execute_reply": "2023-06-08T16:07:21.72057Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# receiver operating characteristic curve",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "pred_positive = pred[:,1]\nfpr, tpr, thresholds = roc_curve(Y_test_label, pred_positive)\nroc_auc = auc(fpr, tpr)\nplt.figure(figsize=(10,10))\nplt.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % roc_auc,color=\"g\",linestyle=\"--\",marker=\"o\",markersize=3,markerfacecolor=\"k\")\n\nplt.plot([0, 1], [0, 1],linestyle=\"--\",linewidth=3,color=\"m\")  # random predictions curve\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.0])\nplt.xlabel('False Positive Rate or (1 - Specifity)')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver Operating Characteristic')\nplt.legend(loc=\"lower right\")\nplt.show()\n\n",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-06-08T16:07:21.723355Z",
     "iopub.execute_input": "2023-06-08T16:07:21.724045Z",
     "iopub.status.idle": "2023-06-08T16:07:21.963414Z",
     "shell.execute_reply.started": "2023-06-08T16:07:21.724004Z",
     "shell.execute_reply": "2023-06-08T16:07:21.96249Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Calculate precision, recall and thresholds\nprecision, recall, thresholds = precision_recall_curve(Y_test_label,pred[:,1])\n\n# Plot the PR curve\nplt.figure(figsize=(10,10))\nplt.step(recall, precision, color='b', alpha=0.2, where='post')\nplt.fill_between(recall, precision, step='post', alpha=0.2, color=\"darkmagenta\")\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.ylim([0.0, 1.05])\nplt.xlim([0.0, 1.0])\nplt.title('Precision-Recall Curve\\n',fontsize=20,color=\"black\")\nplt.xlabel(\"Recall\",fontsize=15,color=\"violet\")\nplt.ylabel(\"Precision\",fontsize=15,color=\"darkgreen\")\nplt.show()\n",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-06-08T16:07:21.965284Z",
     "iopub.execute_input": "2023-06-08T16:07:21.965648Z",
     "iopub.status.idle": "2023-06-08T16:07:22.190504Z",
     "shell.execute_reply.started": "2023-06-08T16:07:21.965596Z",
     "shell.execute_reply": "2023-06-08T16:07:22.189534Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Encode the labels\nlabel_encoder=LabelEncoder()\nlabel_encoder.fit(Y_test_label)\ntest_label_data=label_encoder.transform(Y_test_label)\nclasses=label_encoder.classes_\ncolors = [\"navy\", \"darkorange\"]\nplt.figure(figsize=(10,10))\n\n# Compute the ROC curve and AUC for each class\nfpr = {}\ntpr = {}\nroc_auc = dict()\nfor i, class_ in enumerate(classes):\n    # Binarize the label data\n    label_binarized = (test_label_data == i).astype(int)\n    fpr[i], tpr[i], _ = roc_curve(label_binarized, pred[:, i])\n    roc_auc[i] = auc(fpr[i], tpr[i])\n\n    # Plot the ROC curve for each class\n    plt.plot(fpr[i], tpr[i], label='%s ROC Curve Class  (AUC=%0.2f)' % (class_, roc_auc[i]), marker=\">\", color=colors[i])\n\nplt.plot([0,1],[0,1], marker=\"H\", color=\"k\", linestyle=\"--\", linewidth=4)\nplt.xlim([0,1])\nplt.ylim([0,1.05])\nplt.title('Compute The ROC Curve And AUC For Each Class', color=\"black\", fontsize=15)\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive rate')\nplt.legend(loc='lower right')\nplt.show()\n",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-06-08T16:07:22.191956Z",
     "iopub.execute_input": "2023-06-08T16:07:22.192334Z",
     "iopub.status.idle": "2023-06-08T16:07:22.436538Z",
     "shell.execute_reply.started": "2023-06-08T16:07:22.192294Z",
     "shell.execute_reply": "2023-06-08T16:07:22.435572Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# Compute The Precision Curve And Area Under Curve For Each Class",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Encode the labels\nlabel_encoder=LabelEncoder()\nlabel_encoder.fit(Y_test_label)\ntest_label_data=label_encoder.transform(Y_test_label)\nclasses=label_encoder.classes_\ncolors = [\"darkred\", \"purple\"]\nplt.figure(figsize=(10,10))\n\n# Compute the PR curve and AUC for each class\nprecision = {}\nrecall = {}\npr_auc = dict()\nfor i, class_ in enumerate(classes):\n    # Binarize the label data\n    label_binarized = (test_label_data == i).astype(int)\n    precision[i], recall[i], _ = precision_recall_curve(label_binarized, pred[:, i])\n    pr_auc[i] = auc(recall[i], precision[i])\n\n    # Plot the PR curve for each class\n    plt.plot(recall[i], precision[i], label='%s Precision Curve Class  (AUC=%0.2f)' % (class_, pr_auc[i]),color=colors[i],marker=\"D\",markersize=2,linestyle=\"-.\")\n\nplt.xlim([0,1])\nplt.ylim([0,1.05])\nplt.title('Compute The Precision Curve And Area Under Curve For Each Class\\n',color=\"darkblue\",fontsize=20)\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.legend(loc='lower right')\nplt.show()",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-06-08T16:07:22.438086Z",
     "iopub.execute_input": "2023-06-08T16:07:22.438448Z",
     "iopub.status.idle": "2023-06-08T16:07:22.686638Z",
     "shell.execute_reply.started": "2023-06-08T16:07:22.438413Z",
     "shell.execute_reply": "2023-06-08T16:07:22.68554Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "plt.figure(figsize=(10,10))\n\ncf_matrix=confusion_matrix(Y_test_label,prediction)\nlabel_counts = [\"{0:0.0f}\".format(value) for value in cf_matrix.flatten()]\nlabel_percentages = [\"{0:.2%}\".format(value) for value in cf_matrix.flatten()/np.sum(cf_matrix)]\nlabel_names = [\"True Positive\",\"False Positive\",\"False Negative\",\"True Negative\"]\nlabels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in zip(label_names,label_counts,label_percentages)]\nlabels = np.asarray(labels).reshape(2,2)\nsns.heatmap(cf_matrix, annot=labels, fmt=\"\", cmap='gist_ncar',xticklabels=label_name,yticklabels=label_name)\nplt.title(\"Confusion Matrix\\n\",color=\"gold\",fontsize=20)\nplt.ylabel(\"True Label\\n\",fontsize=15,color=\"yellow\")\nplt.xlabel(\"\\nPredicted Label\",fontsize=15,color=\"yellow\")\nplt.show()",
   "metadata": {
    "id": "m144rJyXpKjT",
    "execution": {
     "iopub.status.busy": "2023-06-08T16:07:22.688166Z",
     "iopub.execute_input": "2023-06-08T16:07:22.688556Z",
     "iopub.status.idle": "2023-06-08T16:07:22.953503Z",
     "shell.execute_reply.started": "2023-06-08T16:07:22.688518Z",
     "shell.execute_reply": "2023-06-08T16:07:22.952547Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "print(classification_report(Y_test_label,prediction,target_names=label_name))",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-06-08T16:07:22.954836Z",
     "iopub.execute_input": "2023-06-08T16:07:22.955774Z",
     "iopub.status.idle": "2023-06-08T16:07:22.974923Z",
     "shell.execute_reply.started": "2023-06-08T16:07:22.955728Z",
     "shell.execute_reply": "2023-06-08T16:07:22.973933Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# Custom Data Prediction\n> * 0==Positive\n> * 1==Racist And Sexist",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "x=\"hank you very much for sharing your experience with us.We are really happy that your interaction with our brand was so positive.I just want to let you know that we are acting upon your feedback to make some vital changes to the way we operate [list of changes].As you can see, the opinions of our clients help us to provide better experiences and grow as a company.Regards,\"\npred_input=roberta_tokenizer.encode_plus(x,add_special_tokens = True,max_length =128,pad_to_max_length = True,truncation=True)\ntest_input_id=np.array(pred_input['input_ids'])\ntest_input_mask=np.array(pred_input['attention_mask'])\n# Get the predicted probabilities\ntext_predict = model.predict([test_input_id.reshape(1,-1), test_input_mask.reshape(1,-1)])\n\n# Get the class with the highest probability\npredicted_class = np.argmax(text_predict, axis=-1)[0]\n\n# Check if the input is toxic (1) or positive (0)\nif predicted_class == 1:\n    print(\"The input text is toxic.\")\nelse:\n    print(\"The input text is positive.\")\n",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-06-08T16:07:22.977363Z",
     "iopub.execute_input": "2023-06-08T16:07:22.978291Z",
     "iopub.status.idle": "2023-06-08T16:07:23.057107Z",
     "shell.execute_reply.started": "2023-06-08T16:07:22.978252Z",
     "shell.execute_reply": "2023-06-08T16:07:23.056156Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ]
}
